{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/c/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/c/anaconda3/lib/python3.6/site-packages/Bio/GA/__init__.py:14: BiopythonDeprecationWarning: Bio.GA has been deprecated, and we intend to remove it in a future release of Biopython. Please consider using DEAP instead.  If you would like to continue using Bio.GA, please contact the Biopython developers via the mailing list or GitHub.\n",
      "  BiopythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras\n",
    "import tensorflow\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding\n",
    "from keras.layers import LSTM, Input, TimeDistributed, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import Bio.GA.Crossover.TwoPoint\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dna.txt', 'r') as myfile:\n",
    "  dna = myfile.read()\n",
    "dna_basepairs = dna\n",
    "dna_basepairs = re.sub(r'[0-9]([0-9])?([0-9])?([0-9])?([0-9])?([0-9])?', '', dna_basepairs)\n",
    "dna_basepairs = re.sub(r'\\n', '', dna_basepairs)\n",
    "dna_basepairs = re.sub(r'\\s+', '', dna_basepairs).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create population of arrays that span 50 index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population(data):\n",
    "    sequence_one = data[:78122]\n",
    "    sequence_two = data[78123:]\n",
    "    \n",
    "    array = np.zeros((len(sequence_one),8))\n",
    "    vectorize_sequence = {\n",
    "        'a' : 1,\n",
    "        't' : 2,\n",
    "        'g' : 3,\n",
    "        'c' : 4\n",
    "    }\n",
    "\n",
    "    for base in range(len(sequence_one)):\n",
    "        base_one = sequence_one[base]\n",
    "        base_two = sequence_two[base]\n",
    "        array[base, vectorize_sequence[base_one]] = 1\n",
    "        array[base, vectorize_sequence[base_two] + 3] = 1\n",
    "        \n",
    "    similarity_array = np.zeros((len(array),1))\n",
    "    similarity_array = np.asarray([similarity_array[i] == 1 if np.equal(array[i, :4], array[i, 4:]).all() else similarity_array[i] == 0 for i in range(len(array))])\n",
    "        \n",
    "    \n",
    "    return [np.asarray(array).reshape(1, 78122, 8), similarity_array.reshape(1, 78122, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = population(dna_basepairs)\n",
    "comparison = sequences[0].reshape(1, 78122, 8)\n",
    "similarity_array = sequences[1].reshape(1, 78122, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 78122, 8)\n",
      "(1, 78122, 1)\n"
     ]
    }
   ],
   "source": [
    "print(comparison.shape)\n",
    "print(similarity_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looping through previous and current index of the population in order to pass each sequence through the RNN for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_population = []\n",
    "for i in range(30):\n",
    "    model = Sequential()\n",
    "    # add Long short term memory RNN and a dense layer. Compile the model\n",
    "    model.add(LSTM(10, input_shape=(comparison.shape[1],8), activation='tanh', return_sequences=True, use_bias=True))\n",
    "    model.add(Dense(1, activation='tanh', use_bias=True))\n",
    "    network_population.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in network_population:\n",
    "    mse_vec = np.zeros(len(network_population))\n",
    "    \n",
    "    for i in range(len(network_population)):\n",
    "        def mse(y_true, y_pred):\n",
    "            y_true = y_true.reshape(1, y_true.shape[1])\n",
    "            y_pred = y_pred.reshape(1, y_pred.shape[1])\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            return mse\n",
    "\n",
    "        y_pred = model.predict(comparison)\n",
    "        mse = mse(similarity_array, y_pred)\n",
    "        mse_vec[i] = mse\n",
    "    print(mse_vec) \n",
    "#     sorted_mse_idx = np.argsort(mse_vec)\n",
    "#     fittest_mse = sorted_mse_idx[:10]\n",
    "#     weakest_mse = sorted_mse_idx[10:]\n",
    "    \n",
    "#     for network in fittest_mse:\n",
    "#         model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validate, test = np.split(sequences, [int(.5*len(sequences)), int(.75*len(sequences))])\n",
    "#view the various lengths of the split data.\n",
    "#print('Train length:', len(train), '\\n', 'Test length:', len(test), '\\n', 'Validation length:', len(validate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
